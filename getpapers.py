import requests
import json
import time  


queries = [
    "deep learning",
    "computer vision",
    "reinforcement learning",
    "transformers",
    "NLP",
    "machine learning",
    "graph neural networks",
    "medical AI",
    "image segmentation",
    "speech recognition"
]


LIMIT = 100                 # nombre d articles par appel
NB_PAGES_PER_QUERY = 10     # nombre de pages par mot cle
BASE_URL = "https://api.semanticscholar.org/graph/v1/paper/search"

all_papers = []

print("ðŸš€ DÃ©but de la collecte...\n")


for q in queries:
    print(f"ðŸ“Œ Collecte pour le thÃ¨me : {q}")
    for page in range(NB_PAGES_PER_QUERY):
        offset = page * LIMIT
        
        params = {
            "query": q,
            "limit": LIMIT,
            "offset": offset,
            "fields": "title"
        }

        try:
            resp = requests.get(BASE_URL, params=params, timeout=10)
            resp.raise_for_status()
            data = resp.json()
            
            # Ajouter les rÃ©sultats
            all_papers.extend(data.get("data", []))
            
            print(f"  âž• Page {page+1}/{NB_PAGES_PER_QUERY} rÃ©cupÃ©rÃ©e")
        
        except Exception as e:
            print(f"âš  Erreur sur {q} page {page+1} â†’ {e}")
        
        # Pause pour Ã©viter rate-limit
        time.sleep(1)

print("\nðŸ“Š FIN DE COLLECTE")
print(f"Nombre total d'articles rÃ©cupÃ©rÃ©s : {len(all_papers)}")

# ðŸ’¾ Sauvegarde
with open("papers_list_raw.json", "w", encoding="utf-8") as f:
    json.dump(all_papers, f, ensure_ascii=False, indent=2)

print("âœ… Fichier 'papers_list_raw.json' crÃ©Ã©.")
